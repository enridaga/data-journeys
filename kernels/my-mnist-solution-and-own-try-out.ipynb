{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import utils as tf_utils\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Lambda\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import regularizers\n\n\n#from keras.models import Sequential\n#from keras.layers import Dense, Dropout, Flatten\n#from keras.callbacks import ModelCheckpoint\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the data..."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"mnist_train_complete = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nmnist_test_complete = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\n\nmnist_train_complete.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preparing the training and testing sets, separating the training pictures of the numbers (i.e. train_x)\n# from their label (i.e train_y).\n# We set here also the data types as int32\ntrain_y = mnist_train_complete.iloc[:, 0].values.astype('int32')\ntrain_x = mnist_train_complete.iloc[:, 1:].values.astype('int32')\ntest_x = mnist_test_complete.values.astype('int32')\n\n# reshaping the training and testing sets to have each digit image of 28 by 28 pixels\ntrain_x = train_x.reshape(train_x.shape[0], 28, 28)\ntest_x = test_x.reshape(test_x.shape[0], 28, 28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing some digit images"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range (10,14):\n    plt.subplot(330 + i+1)\n    plt.imshow(train_x[i], cmap=plt.get_cmap('gray'))\n    plt.title(train_y[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_detail(img, ax):\n    ax.imshow(img, cmap='gray')\n    width, height = img.shape\n    threshold = img.max()/2.5\n    for x in range(width):\n        for y in range(height):\n            ax.annotate(str(round(img[x][y], 2)), xy=(y,x),\n                        horizontalalignment='center',\n                        verticalalignment='center',\n                        color='white' if img[x][y] < threshold else 'black')\nfig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(111)\nvisualize_detail(train_x[10], ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing the training and testing sets\ntrain_x = train_x.astype('float32')/np.max(train_x)\ntest_x = test_x.astype('float32')/np.max(test_x)\n\n# center the normalized data around zero\nmean = np.std(train_x)\ntrain_x -= mean\nmean = np.std(test_x)\ntest_x -= mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating the training and validationg sets\nsplitted_train_X, splitted_test_X, splitted_train_y, splitted_test_y = train_test_split(train_x, train_y, test_size=0.2, random_state=81)\n\n# one-hot encoding the training and validation sets\nohe_splitted_train_y = tf_utils.to_categorical(splitted_train_y, 10)\nohe_splitted_test_y = tf_utils.to_categorical(splitted_test_y, 10)\n\n# print first one-hot training labels\nprint('One-hot labels:')\nprint(splitted_train_y[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Solution 1. Model using fully connected NNs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a fully connected NNs model\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten(input_shape = splitted_train_X.shape[1:]))\nmodel.add(tf.keras.layers.Dense(512, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(512, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n# summary of model\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate test accuracy\nscore = model.evaluate(splitted_test_X, ohe_splitted_test_y, verbose=0)\naccuracy = 100 * score[1]\n\n# print test accuracy\nprint('Test accuracy: %4f%%' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5',\n                               verbose=1, save_best_only=True)\n\nhist = model.fit(splitted_train_X, ohe_splitted_train_y, batch_size=128, epochs=10,\n                 validation_split=0.2, callbacks=[checkpointer],\n                 verbose=1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the weights that yielded the best validation accuracy\nmodel.load_weights('mnist.model.best.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(splitted_test_X, ohe_splitted_test_y, verbose=0)\naccuracy = 100 * score[1]\n\n#print test accuracy\nprint('Test accuracy: %.4f%%' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making predictions using Solution 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_x)\npredictions = [ np.argmax(x) for x in predictions ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare submission\nsubmission = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')\nsubmission.drop('Label', axis=1, inplace=True)\nsubmission['Label'] = predictions\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The prediction obtained by this solution yielded a score of 0.97185. <br>\n**Note**: By centring the normalized training (and testing) data around zero, my score moved forward by 83 places on the leaderboard (previous score was 0.96800)."},{"metadata":{},"cell_type":"markdown","source":"# Solution 2. Model using Convolutional NNs"},{"metadata":{},"cell_type":"markdown","source":"In this solution I implement a Convolutional Network to replace my Fully Connected Neural Network from solution1."},{"metadata":{"trusted":true},"cell_type":"code","source":"extended_splitted_train_X = splitted_train_X[..., tf.newaxis]\nextended_splitted_test_X = splitted_test_X[..., tf.newaxis]\nextended_splitted_test_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a Convolutional NNs model\nmodel = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=extended_splitted_train_X.shape[1:]))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\n\n# Converts our 3D feature maps to 1D features vectors\nmodel.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\n\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate test accuracy\nscore = model.evaluate(extended_splitted_test_X, ohe_splitted_test_y, verbose=0)\naccuracy = 100 * score[1]\n\n# print test accuracy\nprint('Test accuracy: %4f%%' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(extended_splitted_train_X, ohe_splitted_train_y, batch_size=128, epochs=10,\n                 validation_split=0.2, #callbacks=[checkpointer], AttributeError: 'Sequential' object has no attribute '_in_multi_worker_mode'\n                 verbose=1, validation_data=(extended_splitted_test_X, ohe_splitted_test_y), shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(extended_splitted_test_X, ohe_splitted_test_y, verbose=0)\naccuracy = 100 * score[1]\n\n#print test accuracy\nprint('Test accuracy: %.4f%%' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making predictions using Solution 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extend the test imagae set with an additional dimension\nextended_test_x = test_x[..., tf.newaxis]\npredictions = model.predict(extended_test_x)\npredictions = [ np.argmax(x) for x in predictions ]\n\n# prepare submission\nsubmission = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')\nsubmission.drop('Label', axis=1, inplace=True)\nsubmission['Label'] = predictions\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The prediction obtained by this solution yielded a score of 0.98714."},{"metadata":{},"cell_type":"markdown","source":"# Solution 3. Model using Convolutional NNs with data augmentation"},{"metadata":{},"cell_type":"markdown","source":"In this solution I make use of the model architecture of solution number 2 but I also implement data augmentation for the training."},{"metadata":{},"cell_type":"markdown","source":"## Augmenting an image"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a data augmentator for our images\nimage_augmentator = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    # rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.1,\n    fill_mode='nearest')\n\n# define size of batch\nbatch_size = 32\n\ntrain_batches = image_augmentator.flow(extended_splitted_train_X, ohe_splitted_train_y, batch_size=batch_size)\nval_batches = image_augmentator.flow(extended_splitted_test_X, ohe_splitted_test_y, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at an example of data augmentation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"example_img = train_x[10][..., tf.newaxis]\ntransf_params = { 'theta':15., 'tx':0.1, 'ty':0.1, 'shear':0.2 }\naugmented_image = image_augmentator.apply_transform(example_img, transf_params)\n\n# reducing dimensinoality to two\ntwoDim_image = augmented_image[:, :, 0]\n\nfig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(111)\nvisualize_detail(twoDim_image, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a Convolutional NNs model (solution number 3)\nmodel = Sequential()\nmodel.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=extended_splitted_train_X.shape[1:]))\n#model.add(MaxPooling2D(pool_size=2))\nmodel.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.1))\nmodel.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\n\n# Converts our 3D feature maps to 1D features vectors\nmodel.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\n\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(generator=train_batches, steps_per_epoch =train_batches.n, epochs=1,\n                    validation_data=val_batches, validation_steps=val_batches.n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(extended_splitted_test_X, ohe_splitted_test_y, verbose=0)\naccuracy = 100 * score[1]\n\n#print test accuracy\nprint('Test accuracy: %.4f%%' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making predictions using Solution 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extend the test imagae set with an additional dimension\nextended_test_x = test_x[..., tf.newaxis]\npredictions = model.predict(extended_test_x)\npredictions = [ np.argmax(x) for x in predictions ]\n\n# prepare submission\nsubmission = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')\nsubmission.drop('Label', axis=1, inplace=True)\nsubmission['Label'] = predictions\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The prediction obtained by this solution yielded a score of 0.98771.\nI advanced 43 places on the leaderboard."},{"metadata":{},"cell_type":"markdown","source":"# Solution 4. Model using Convolutional NNs with data augmentation and batch normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a Convolutional NNs model (solution number 4)\nmodel = Sequential()\nmodel.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=extended_splitted_train_X.shape[1:]))\n#model.add(MaxPooling2D(pool_size=2))\nBatchNormalization(axis=1)\nmodel.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.1))\nBatchNormalization(axis=1)\nmodel.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\n\n# Converts our 3D feature maps to 1D features vectors\nmodel.add(Flatten())\nBatchNormalization()\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nBatchNormalization()\nmodel.add(Dense(10, activation='softmax'))\n\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(generator=train_batches, steps_per_epoch =train_batches.n, epochs=1,# 2000 // batch_size, epochs=50,\n                    validation_data=val_batches, validation_steps=val_batches.n)#800 // batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(extended_splitted_test_X, ohe_splitted_test_y, verbose=0)\naccuracy = 100 * score[1]\n\n#print test accuracy\nprint('Test accuracy: %.4f%%' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start all over again, but with more filters on each convolutional layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a Convolutional NNs model (same architecture as in solution number 4)\nmodel = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=extended_splitted_train_X.shape[1:]))\n#model.add(MaxPooling2D(pool_size=2))\nBatchNormalization(axis=1)\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.1))\nBatchNormalization(axis=1)\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\n#model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n\n# Converts our 3D feature maps to 1D features vectors\nmodel.add(Flatten())\nBatchNormalization()\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nBatchNormalization()\nmodel.add(Dense(10, activation='softmax'))\n\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add another dimension to the training data\nfinal_train_x = train_x[..., tf.newaxis]\n# one hot encode the complete training labels\nfinal_train_y = tf_utils.to_categorical(train_y, 10)\n\n#model.optimizer.learning_rate=0.01\nbatches = image_augmentator.flow(final_train_x, final_train_y, batch_size=64)\n\nhistory = model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1)# 2000 // batch_size, epochs=50,\n                    #validation_data=val_batches, validation_steps=val_batches.n)#800 // batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extended_test_x = test_x[..., tf.newaxis]\npredictions = model.predict(extended_test_x)\npredictions = [ np.argmax(x) for x in predictions ]\n\n# prepare submission\nsubmission = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')\nsubmission.drop('Label', axis=1, inplace=True)\nsubmission['Label'] = predictions\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this last try I got a score of 0.98257 which is actually not an improvement to my last score.\n:("},{"metadata":{},"cell_type":"markdown","source":"# Solution 5. Adding a Lambda layer to my last CNN"},{"metadata":{},"cell_type":"markdown","source":"I will try again, but this time I will add a Lambda layer at the input of my NN. This layer input will center the data around zero mean and unite variance (I got this from [Poonam Ligade's notebook](https://www.kaggle.com/poonaml/deep-neural-network-keras-way/notebook)). This means I have to take again the original data (not preprocessed data), and add another dimension. The Lambda layer will perform a \"Standardize\" function (defined later some blocks below) which will do the preprocessing to each one of the images (i.e. as mentioned before, center the data around zero mean and unit variance)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create new datasets out of the original files provided by kaggle (to avoid confussions with other\n# variables created in other sections of this notebook and because I need this data without the first preprocessing steps\n# I performed in my previous solutions)\ntrain_y_sol5 = mnist_train_complete.iloc[:, 0].values.astype('int32')\ntrain_x_sol5 = mnist_train_complete.iloc[:, 1:].values.astype('int32')\ntrain_x_sol5 = train_x_sol5.reshape(train_x_sol5.shape[0], 28, 28,1)\n\n# create new datasets out of the original files provided by kaggle (to avoid confussions with other\n# variables created in other sections of this notebook and because I need this data without the first preprocessing steps\n# I performed in my previous solutions)\ntest_x_sol5 =  mnist_test_complete.values.astype('int32')\ntest_x_sol5 =  test_x_sol5.reshape(test_x_sol5.shape[0], 28, 28,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation\ns5_train_x, s5_test_x, s5_train_y, s5_test_y = train_test_split(train_x_sol5, train_y_sol5,\n                                                                test_size=0.2,\n                                                                random_state=81)\n# one-hot encoding the target labels\nohe_s5_train_y = tf_utils.to_categorical(s5_train_y, 10)\nohe_s5_test_y = tf_utils.to_categorical(s5_test_y, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create new image generators using the same image_augmentator created previously,\n# but with a different number of batches (prevous batch size was 32).\ntrain_batches_sol5 = image_augmentator.flow(s5_train_x, ohe_s5_train_y, batch_size=64)\nval_batches_sol5 = image_augmentator.flow(s5_test_x, ohe_s5_test_y, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new preprocessing of data (to be applied to each individual image by the Lamda layer)\nmean_px = train_x_sol5.mean().astype(np.float32)\nstd_px = train_x_sol5.std().astype(np.float32)\n\n# define the function that will be performed by our Lambda layer on each of the input images\ndef standardize(x): \n    return (x-mean_px)/std_px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a Convolutional NNs model (same architecture as in solution number 4)\nmodel = Sequential()\nmodel.add(Lambda(standardize, input_shape=(28,28,1)))\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'#, \n                 kernel_regularizer=regularizers.l2(0.01),\n                 #activity_regularizer=regularizers.l2(0.01),\n                 ))\n\nBatchNormalization(axis=1)\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'#,\n                 #kernel_regularizer=regularizers.l2(0.01)\n                 #activity_regularizer=regularizers.l2(0.01)\n                ))\nmodel.add(MaxPooling2D(pool_size=2))\n\nBatchNormalization(axis=1)\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu',\n                 #kernel_regularizer=regularizers.l2(0.01)\n                 #activity_regularizer=regularizers.l1(0.01)\n         ))\nmodel.add(MaxPooling2D(pool_size=2))\n#model.add(Dropout(0.1))\n\n# Converts our 3D feature maps to 1D features vectors\nmodel.add(Flatten())\nBatchNormalization()\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.1))\nBatchNormalization()\nmodel.add(Dense(10, activation='softmax'))\n\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(generator=train_batches_sol5, steps_per_epoch =train_batches_sol5.n, epochs=1,# 2000 // batch_size, epochs=50,\n                    validation_data=val_batches_sol5, validation_steps=val_batches_sol5.n)#800 // batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(s5_test_x, ohe_s5_test_y, verbose=0)\naccuracy = 100 * score[1]\n\n#print test accuracy\nprint('Test accuracy: %.4f%%' % accuracy) #97.9167","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.optimizer.lerning_rate=0.01\n#gen = ImageDataGenerator()\n#batches = gen.flow(train_x_sol5, tf_utils.to_categorical(train_y_sol5, 10), batch_size=64)\n#history=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extended_test_x = test_x[..., tf.newaxis]\npredictions = model.predict(extended_test_x)\npredictions = [ np.argmax(x) for x in predictions ]\n\n# prepare submission\nsubmission = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')\nsubmission.drop('Label', axis=1, inplace=True)\nsubmission['Label'] = predictions\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"## Changes made on my last Neural Network"},{"metadata":{},"cell_type":"markdown","source":"Instead of preprocessing the complete set of images by scaling them and trying to center them on zero mean (using the max value and the mean value of the complete dataset as done in my [small preprocessing section](#Preprocessing)), I did a zero mean centering with unit variance on each one of the images by means of defining a \"standardize\" function that would be executed by a Lambda layer (placed at the begining / top of my neural network). I also added [Ridge regression](https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db) to the first convolutional layer of my neural network in an attempt to penalize more those feature of the images that does not help the algorithm to improve during its training. Additionally I removed the dropout layer from the convolutional part of my architecture and left only a dropout layer between the two Dense layers at the end of the architecture (lowering the percentage of dropout to 10% in comparison to the same dropout layer in my previous solutions).\nFinally, I did the training of this last model in two steps being the first step a training on the cross validation data sets with data augmentation and the second step being a training on the complete training dataset provided by Kaggle, without data augmentation and adjusting a new learning rate value of 10 % in our model / NN."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}