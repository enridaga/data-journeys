digraph "" {
	numpy -> "applying-text-mining.ipynb"	[label=importedBy];
	"np(0)" -> numpy	[label=assignedFrom];
	"np(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	pandas -> "applying-text-mining.ipynb"	[label=importedBy];
	"pd(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"pd(0)" -> pandas	[label=assignedFrom];
	os -> "applying-text-mining.ipynb"	[label=importedBy];
	"os(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"os(0)" -> os	[label=assignedFrom];
	"print[13/0]" -> "os(0)"	[label=print];
	"print[13/0]" -> "../input(0)"	[label=print];
	"../input(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"No woman no cry(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"text(0)$0" -> "No woman no cry(0)"	[label=assignedFrom];
	"length of text: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[20/0]" -> "text(0)$0"	[label=print];
	"print[20/0]" -> "length of text: (0)"	[label=print];
	"print[20/0]" -> "len(0)"	[label=print];
	"len(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"splitted_text(0)$0" -> "text(0)$0"	[label=split];
	"Splitted text: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[24/0]" -> "splitted_text(0)$0"	[label=print];
	"print[24/0]" -> "Splitted text: (0)"	[label=print];
	"word(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"specific_words(0)$0" -> "len(0)"	[label=assignedFrom];
	"specific_words(0)$0" -> "splitted_text(0)$0"	[label=assignedFrom];
	"specific_words(0)$0" -> "word(0)"	[label=assignedFrom];
	"specific_words(0)$0" -> "2(0)"	[label=assignedFrom];
	"2(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"Words which are more than 3 letter: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[29/0]" -> "specific_words(0)$0"	[label=print];
	"print[29/0]" -> "Words which are more than 3 letter: (0)"	[label=print];
	"capital_words(0)$0" -> "splitted_text(0)$0"	[label=assignedFrom];
	"capital_words(0)$0" -> "word(0)"	[label=assignedFrom];
	"Capitalized words: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[33/0]" -> "capital_words(0)$0"	[label=print];
	"print[33/0]" -> "Capitalized words: (0)"	[label=print];
	"words_end_with_o(0)$0" -> "splitted_text(0)$0"	[label=assignedFrom];
	"words_end_with_o(0)$0" -> "word(0)"	[label=assignedFrom];
	"words_end_with_o(0)$0" -> "o(0)"	[label=assignedFrom];
	"o(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"words end with o: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[37/0]" -> "words_end_with_o(0)$0"	[label=print];
	"print[37/0]" -> "words end with o: (0)"	[label=print];
	"words_start_with_w(0)$0" -> "splitted_text(0)$0"	[label=assignedFrom];
	"words_start_with_w(0)$0" -> "word(0)"	[label=assignedFrom];
	"words_start_with_w(0)$0" -> "w(0)"	[label=assignedFrom];
	"w(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"words start with w: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[41/0]" -> "words_start_with_w(0)$0"	[label=print];
	"print[41/0]" -> "words start with w: (0)"	[label=print];
	"unique words: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[43/0]" -> "splitted_text(0)$0"	[label=print];
	"print[43/0]" -> "unique words: (0)"	[label=print];
	"print[43/0]" -> "set(0)"	[label=print];
	"set(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"lowercase_text(0)$0" -> "splitted_text(0)$0"	[label=assignedFrom];
	"lowercase_text(0)$0" -> "word(0)"	[label=assignedFrom];
	"print[49/0]" -> "unique words: (0)"	[label=print];
	"print[49/0]" -> "set(0)"	[label=print];
	"print[49/0]" -> "lowercase_text(0)$0"	[label=print];
	"Is w letter in woman word:(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[51/0]" -> "w(0)"	[label=print];
	"print[51/0]" -> "Is w letter in woman word:(0)"	[label=print];
	"print[51/0]" -> "woman(0)"	[label=print];
	"woman(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"Is word uppercase:(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[54/0]" -> "Is word uppercase:(0)"	[label=print];
	"print[54/0]" -> "WOMAN(0)"	[label=print];
	"WOMAN(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"Is word lowercase:(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[55/0]" -> "Is word lowercase:(0)"	[label=print];
	"print[55/0]" -> "cry(0)"	[label=print];
	"cry(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"Is word made of by digits: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[58/0]" -> "Is word made of by digits: (0)"	[label=print];
	"print[58/0]" -> "12345(0)"	[label=print];
	"12345(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"00000000No cry: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[61/0]" -> "00000000No cry: (0)"	[label=print];
	"print[61/0]" -> "00000000No cry(0)"	[label=print];
	"print[61/0]" -> "0(0)"	[label=print];
	"00000000No cry(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"0(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"Find particular letter from back: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[64/0]" -> "o(0)"	[label=print];
	"print[64/0]" -> "Find particular letter from back: (0)"	[label=print];
	"print[64/0]" -> "No cry no(0)"	[label=print];
	"No cry no(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[67/0]" -> "o(0)"	[label=print];
	"print[67/0]" -> "Find particular letter from back: (0)"	[label=print];
	"print[67/0]" -> "No cry no(0)"	[label=print];
	"Replace o with 3 (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[70/0]" -> "o(0)"	[label=print];
	"print[70/0]" -> "No cry no(0)"	[label=print];
	"print[70/0]" -> "Replace o with 3 (0)"	[label=print];
	"print[70/0]" -> "3(0)"	[label=print];
	"3(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"Each letter: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[73/0]" -> "Each letter: (0)"	[label=print];
	"print[73/0]" -> "list(0)"	[label=print];
	"print[73/0]" -> "No cry(0)"	[label=print];
	"list(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"No cry(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"    Be fair and tolerant    (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"text1(0)$0" -> "    Be fair and tolerant    (0)"	[label=assignedFrom];
	"Split text: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[76/0]" -> "text1(0)$0"	[label=print];
	"print[76/0]" -> "Split text: (0)"	[label=print];
	"print[76/0]" -> " (0)"	[label=print];
	" (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"Cleaned text: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[79/0]" -> "text1(0)$0"	[label=print];
	"print[79/0]" -> " (0)"	[label=print];
	"print[79/0]" -> "Cleaned text: (0)"	[label=print];
	"../input/religious-and-philosophical-texts/35895-0.txt(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"f(0)$0" -> "../input/religious-and-philosophical-texts/35895-0.txt(0)"	[label=open];
	"f(0)$0" -> "r(0)"	[label=open];
	"r(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[84/0]" -> "f(0)$0"	[label=print];
	"text3(0)$0" -> "f(0)$0"	[label=read];
	"Length of text: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[88/0]" -> "len(0)"	[label=print];
	"print[88/0]" -> "text3(0)$0"	[label=print];
	"print[88/0]" -> "Length of text: (0)"	[label=print];
	"lines(0)$0" -> "text3(0)$0"	[label=splitlines];
	"Number of lines: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[92/0]" -> "len(0)"	[label=print];
	"print[92/0]" -> "lines(0)$0"	[label=print];
	"print[92/0]" -> "Number of lines: (0)"	[label=print];
	"data(0)$0" -> "pd(0)"	[label=read_csv];
	"data(0)$0" -> "../input/ben-hamners-tweets/benhamner.csv(0)"	[label=read_csv];
	"data(0)$0" -> "latin-1(0)"	[label=read_csv];
	"../input/ben-hamners-tweets/benhamner.csv(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"latin-1(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"data(0)$1" -> "data(0)$0"	[label=head];
	"In his tweets, the rate of occuring kaggle word is: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[99/0]" -> "len(0)"	[label=print];
	"print[99/0]" -> "data(0)$1"	[label=print];
	"print[99/0]" -> "In his tweets, the rate of occuring kaggle word is: (0)"	[label=print];
	"print[99/0]" -> "sum(0)"	[label=print];
	"print[99/0]" -> "kaggle(0)"	[label=print];
	"sum(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"kaggle(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"text(0)$1" -> "data(0)$1"	[label=assignedFrom];
	"text(0)$1" -> "1(0)"	[label=assignedFrom];
	"1(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[102/0]" -> "text(0)$1"	[label=print];
	re -> "applying-text-mining.ipynb"	[label=importedBy];
	"re(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"re(0)" -> re	[label=assignedFrom];
	"callouts(0)$0" -> "word(0)"	[label=assignedFrom];
	"callouts(0)$0" -> " (0)"	[label=assignedFrom];
	"callouts(0)$0" -> "text(0)$1"	[label=assignedFrom];
	"callouts(0)$0" -> "re(0)"	[label=assignedFrom];
	"callouts(0)$0" -> "@[A-Za-z0-9_]+(0)"	[label=assignedFrom];
	"@[A-Za-z0-9_]+(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"callouts: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[108/0]" -> "callouts(0)$0"	[label=print];
	"print[108/0]" -> "callouts: (0)"	[label=print];
	"callouts1(0)$0" -> "word(0)"	[label=assignedFrom];
	"callouts1(0)$0" -> " (0)"	[label=assignedFrom];
	"callouts1(0)$0" -> "text(0)$1"	[label=assignedFrom];
	"callouts1(0)$0" -> "re(0)"	[label=assignedFrom];
	"callouts1(0)$0" -> "@\w+(0)"	[label=assignedFrom];
	"@\w+(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[113/0]" -> "callouts: (0)"	[label=print];
	"print[113/0]" -> "callouts1(0)$0"	[label=print];
	"print[115/0]" -> "text(0)$1"	[label=print];
	"print[115/0]" -> "re(0)"	[label=print];
	"print[115/0]" -> "[w](0)"	[label=print];
	"[w](0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[119/0]" -> "text(0)$1"	[label=print];
	"print[119/0]" -> "re(0)"	[label=print];
	"print[119/0]" -> "[^w](0)"	[label=print];
	"[^w](0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"15-10-2000
09/10/2005
15-05-1999
05/05/99

05/05/199

05/05/9(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"date(0)$0" -> "15-10-2000
09/10/2005
15-05-1999
05/05/99

05/05/199

05/05/9(0)"	[label=assignedFrom];
	"re(0)$0" -> "re(0)"	[label=findall];
	"re(0)$0" -> "date(0)$0"	[label=findall];
	"re(0)$0" -> "\d{1,2}[/-]\d{1,2}[/-]\d{1,4}(0)"	[label=findall];
	"\d{1,2}[/-]\d{1,2}[/-]\d{1,4}(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	nltk -> "applying-text-mining.ipynb"	[label=importedBy];
	"nlp(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"nlp(0)" -> nltk	[label=assignedFrom];
	"text(0)$2" -> "data(0)$1"	[label=assignedFrom];
	"text(0)$2" -> "1(0)"	[label=assignedFrom];
	"splitted(0)$0" -> " (0)"	[label=split];
	"splitted(0)$0" -> "text(0)$2"	[label=split];
	"number of words: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[129/0]" -> "len(0)"	[label=print];
	"print[129/0]" -> "splitted(0)$0"	[label=print];
	"print[129/0]" -> "number of words: (0)"	[label=print];
	"text(0)$3" -> "data(0)$1"	[label=assignedFrom];
	"text(0)$3" -> "1(0)"	[label=assignedFrom];
	"number of unique words: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[133/0]" -> "len(0)"	[label=print];
	"print[133/0]" -> "set(0)"	[label=print];
	"print[133/0]" -> "splitted(0)$0"	[label=print];
	"print[133/0]" -> "number of unique words: (0)"	[label=print];
	"first 5 unique words: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[136/0]" -> "set(0)"	[label=print];
	"print[136/0]" -> "list(0)"	[label=print];
	"print[136/0]" -> "splitted(0)$0"	[label=print];
	"print[136/0]" -> "first 5 unique words: (0)"	[label=print];
	"print[136/0]" -> "5(0)"	[label=print];
	"5(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"dist(0)$0" -> "nlp(0)"	[label=FreqDist];
	"dist(0)$0" -> "splitted(0)$0"	[label=FreqDist];
	"frequency of words: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[140/0]" -> "dist(0)$0"	[label=print];
	"print[140/0]" -> "frequency of words: (0)"	[label=print];
	"words in text: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[143/0]" -> "dist(0)$0"	[label=print];
	"print[143/0]" -> "words in text: (0)"	[label=print];
	"the word box is occured how many times:(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[146/0]" -> "dist(0)$0"	[label=print];
	"print[146/0]" -> "the word box is occured how many times:(0)"	[label=print];
	"print[146/0]" -> "box(0)"	[label=print];
	"box(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"task Tasked tasks tasking(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"words(0)$0" -> "task Tasked tasks tasking(0)"	[label=assignedFrom];
	"words_list(0)$0" -> " (0)"	[label=split];
	"words_list(0)$0" -> "words(0)$0"	[label=split];
	"normalized words: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[150/0]" -> "words_list(0)$0"	[label=print];
	"print[150/0]" -> "normalized words: (0)"	[label=print];
	"porter_stemmer(0)$0" -> "nlp(0)"	[label=PorterStemmer];
	"roots(0)$0" -> "words_list(0)$0"	[label=assignedFrom];
	"roots(0)$0" -> "porter_stemmer(0)$0"	[label=assignedFrom];
	"roots(0)$0" -> "each(0)"	[label=assignedFrom];
	"each(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"roots of task Tasked tasks tasking: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[155/0]" -> "roots(0)$0"	[label=print];
	"print[155/0]" -> "roots of task Tasked tasks tasking: (0)"	[label=print];
	"[<_ast.Constant object at 0x7fd5008b0880>, <_ast.Constant object at 0x7fd5008b0a00>, <_ast.Constant object at 0x7fd5008b0c10>, <_\
ast.Constant object at 0x7fd5008b0e80>, <_ast.Constant object at 0x7fd5008b0e50>](0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"stemming_word_list(0)$0" -> "[<_ast.Constant object at 0x7fd5008b0880>, <_ast.Constant object at 0x7fd5008b0a00>, <_ast.Constant object at 0x7fd5008b0c10>, <_\
ast.Constant object at 0x7fd5008b0e80>, <_ast.Constant object at 0x7fd5008b0e50>](0)"	[label=assignedFrom];
	"porter_stemmer(0)$1" -> "nlp(0)"	[label=PorterStemmer];
	"roots(0)$1" -> "each(0)"	[label=assignedFrom];
	"roots(0)$1" -> "stemming_word_list(0)$0"	[label=assignedFrom];
	"roots(0)$1" -> "porter_stemmer(0)$1"	[label=assignedFrom];
	"result of stemming: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[160/0]" -> "roots(0)$1"	[label=print];
	"print[160/0]" -> "result of stemming: (0)"	[label=print];
	"lemma(0)$0" -> "nlp(0)"	[label=WordNetLemmatizer];
	"lemma_roots(0)$0" -> "each(0)"	[label=assignedFrom];
	"lemma_roots(0)$0" -> "stemming_word_list(0)$0"	[label=assignedFrom];
	"lemma_roots(0)$0" -> "lemma(0)$0"	[label=assignedFrom];
	"result of lemmatization: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[165/0]" -> "lemma_roots(0)$0"	[label=print];
	"print[165/0]" -> "result of lemmatization: (0)"	[label=print];
	"You’re in the right place!(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"text_t(0)$0" -> "You’re in the right place!(0)"	[label=assignedFrom];
	"split the sentece: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[167/0]" -> " (0)"	[label=print];
	"print[167/0]" -> "text_t(0)$0"	[label=print];
	"print[167/0]" -> "split the sentece: (0)"	[label=print];
	"tokenize with nltk: (0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[170/0]" -> "nlp(0)"	[label=print];
	"print[170/0]" -> "text_t(0)$0"	[label=print];
	"print[170/0]" -> "tokenize with nltk: (0)"	[label=print];
	"data(0)$2" -> "pd(0)"	[label=read_csv];
	"data(0)$2" -> "../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv(0)"	[label=read_csv];
	"data(0)$2" -> "latin1(0)"	[label=read_csv];
	"../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"latin1(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"data(0)$3" -> "pd(0)"	[label=concat];
	"data(0)$3" -> "1(0)"	[label=concat];
	"data(0)$3" -> "[<_ast.Attribute object at 0x7fd500836190>, <_ast.Attribute object at 0x7fd500836280>](0)"	[label=concat];
	"[<_ast.Attribute object at 0x7fd500836190>, <_ast.Attribute object at 0x7fd500836280>](0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"data(0)$4" -> "data(0)$3"	[label=dropna];
	"data(0)$5" -> "0(0)"	[label=assignedFrom];
	"data(0)$5" -> "1(0)"	[label=assignedFrom];
	"data(0)$5" -> "each(0)"	[label=assignedFrom];
	"data(0)$5" -> "data(0)$4"	[label=assignedFrom];
	"data(0)$5" -> "data(0)$5"	[label=assignedFrom];
	"data(0)$5" -> "female(0)"	[label=assignedFrom];
	"female(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"sklearn.feature_extraction.text" -> "applying-text-mining.ipynb"	[label=importedBy];
	CountVectorizer -> "sklearn.feature_extraction.text"	[label=importedBy];
	"CountVectorizer(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"CountVectorizer(0)" -> CountVectorizer	[label=assignedFrom];
	"150(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"max_features(0)$0" -> "150(0)"	[label=assignedFrom];
	"english(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"count_vectorizer(0)$0" -> "max_features(0)$0"	[label=CountVectorizer];
	"count_vectorizer(0)$0" -> "english(0)"	[label=CountVectorizer];
	"sparce_matrix(0)$0" -> "count_vectorizer(0)$0"	[label=toarray];
	"sparce_matrix(0)$0" -> "review_list(0)"	[label=toarray];
	"review_list(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"Most used {} words: {}(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"print[200/0]" -> "max_features(0)$0"	[label=print];
	"print[200/0]" -> "count_vectorizer(0)$0"	[label=print];
	"print[200/0]" -> "Most used {} words: {}(0)"	[label=print];
	"y(0)$0" -> "0(0)"	[label=assignedFrom];
	"y(0)$0" -> "data(0)$5"	[label=assignedFrom];
	"sklearn.model_selection" -> "applying-text-mining.ipynb"	[label=importedBy];
	train_test_split -> "sklearn.model_selection"	[label=importedBy];
	"train_test_split(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"train_test_split(0)" -> train_test_split	[label=assignedFrom];
	"x_train(0)$0" -> "0(0)"	[label=train_test_split];
	"x_train(0)$0" -> "sparce_matrix(0)$0"	[label=train_test_split];
	"x_train(0)$0" -> "y(0)$0"	[label=train_test_split];
	"x_train(0)$0" -> "0.1(0)"	[label=train_test_split];
	"0.1(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"x_test(0)$0" -> "0(0)"	[label=train_test_split];
	"x_test(0)$0" -> "sparce_matrix(0)$0"	[label=train_test_split];
	"x_test(0)$0" -> "y(0)$0"	[label=train_test_split];
	"x_test(0)$0" -> "0.1(0)"	[label=train_test_split];
	"y_train(0)$0" -> "0(0)"	[label=train_test_split];
	"y_train(0)$0" -> "sparce_matrix(0)$0"	[label=train_test_split];
	"y_train(0)$0" -> "y(0)$0"	[label=train_test_split];
	"y_train(0)$0" -> "0.1(0)"	[label=train_test_split];
	"y_test(0)$0" -> "0(0)"	[label=train_test_split];
	"y_test(0)$0" -> "sparce_matrix(0)$0"	[label=train_test_split];
	"y_test(0)$0" -> "y(0)$0"	[label=train_test_split];
	"y_test(0)$0" -> "0.1(0)"	[label=train_test_split];
	"sklearn.naive_bayes" -> "applying-text-mining.ipynb"	[label=importedBy];
	GaussianNB -> "sklearn.naive_bayes"	[label=importedBy];
	"GaussianNB(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"GaussianNB(0)" -> GaussianNB	[label=assignedFrom];
	"nb(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"nb(0)$0" -> "sparce_matrix(0)$0"	[label=fit];
	"nb(0)$0" -> "y(0)$0"	[label=fit];
	"nb(0)$0" -> "nb(0)"	[label=fit];
	"y_pred(0)$0" -> "sparce_matrix(0)$0"	[label=predict];
	"y_pred(0)$0" -> "nb(0)$0"	[label=predict];
	"sklearn.metrics" -> "applying-text-mining.ipynb"	[label=importedBy];
	confusion_matrix -> "sklearn.metrics"	[label=importedBy];
	"confusion_matrix(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"confusion_matrix(0)" -> confusion_matrix	[label=assignedFrom];
	"cm(0)$0" -> "y(0)$0"	[label=confusion_matrix];
	"cm(0)$0" -> "y_pred(0)$0"	[label=confusion_matrix];
	"matplotlib.pyplot" -> "applying-text-mining.ipynb"	[label=importedBy];
	"plt(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"plt(0)" -> "matplotlib.pyplot"	[label=assignedFrom];
	seaborn -> "applying-text-mining.ipynb"	[label=importedBy];
	"sns(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
	"sns(0)" -> seaborn	[label=assignedFrom];
	"f(0)$1" -> "5(0)"	[label=subplots];
	"f(0)$1" -> "plt(0)"	[label=subplots];
	"ax(0)$0" -> "5(0)"	[label=subplots];
	"ax(0)$0" -> "plt(0)"	[label=subplots];
	"sns(0)$0" -> "cm(0)$0"	[label=heatmap];
	"sns(0)$0" -> "sns(0)"	[label=heatmap];
	"plt(0)$0" -> "plt(0)"	[label=show];
	"plt(0)$1" -> "plt(0)$0"	[label=savefig];
	"plt(0)$1" -> "graph.png(0)"	[label=savefig];
	"graph.png(0)" -> "applying-text-mining.ipynb"	[label=appearsIn];
}
