{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Datanode graphs to Data Journeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "indir = \"./rdf\"\n",
    "outdir = \"./datajourneys\"\n",
    "didir = \"./graphs\"\n",
    "classifier = \"MLPClassifier_2_1000_rdf2vec.clf\"# \"LogisticRegression_2_200_rdf2vec.clf\"#\"MLPClassifier_2_200_rdf2vec.clf\"\n",
    "rdfgraph = \"rdfgraph_2_1000_rdf2vec.ttl\" #\"rdfgraph_2_200_rdf2vec.ttl\"\n",
    "notebook = 'eda-feature-engineering-lgb-xgb-cat' # 'random-forests' # 'very-simple-pytorch-training-0-59' #'transfer-learning' #'very-simple-pytorch-training-0-59' #'1-quick-start-read-csv-and-flatten-json-fields' \n",
    "rdf_folder = './rdf'\n",
    "clf_folder = './models'\n",
    "use_rules = True\n",
    "djo = \"http://purl.org/datajourneys/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook = notebook[:-6]\n",
    "# print(notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import datajourney as DJ\n",
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS\n",
    "from rdflib import Namespace\n",
    "from rdflib import URIRef, BNode, Literal\n",
    "\n",
    "import networkx.drawing, networkx.drawing.nx_agraph as ag\n",
    "\n",
    "from pyrdf2vec import RDF2VecTransformer\n",
    "from pyrdf2vec.embedders import Word2Vec\n",
    "from pyrdf2vec.graphs import KG, Vertex\n",
    "from pyrdf2vec.walkers import RandomWalker, Walker\n",
    "\n",
    "# import pygraphviz\n",
    "from graphviz import Source\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from time import time, monotonic\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kg(graph, label_predicates):\n",
    "    kg = KG(location='emptygraph.ttl')\n",
    "    for (s, p, o) in graph:\n",
    "        if p not in label_predicates:\n",
    "            s_v = Vertex(str(s))\n",
    "            o_v = Vertex(str(o))\n",
    "            p_v = Vertex(str(p), predicate=True, vprev=s_v, vnext=o_v)\n",
    "            kg.add_vertex(s_v)\n",
    "            kg.add_vertex(p_v)\n",
    "            kg.add_vertex(o_v)\n",
    "            kg.add_edge(s_v, p_v)\n",
    "            kg.add_edge(p_v, o_v)\n",
    "    return kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNotebooks(indir):\n",
    "    # Load general graph and add notebook\n",
    "    rdf_graph = rdflib.Graph()\n",
    "    rdf_graph.parse(clf_folder + \"/\" + rdfgraph, format=\"ttl\")\n",
    "    return rdf_graph\n",
    "\n",
    "def loadKG(rdf_graph):\n",
    "    # rdf_graph.parse(indir + '/' + notebook + '.ttl' , format=\"ttl\")\n",
    "    # Remove rdfs labels before creating the KG\n",
    "    label_predicates = [\"http://www.w3.org/2000/01/rdf-schema#label\"] #skip labels\n",
    "    kg = create_kg(rdf_graph, label_predicates)\n",
    "    return kg\n",
    "\n",
    "def loadNotebook(indir, kg, notebook, use_rules):\n",
    "    # Load RDF notebook\n",
    "    rdf_nb = rdflib.Graph()\n",
    "    rdf_nb.parse(indir + '/' + notebook + '.ttl' , format=\"ttl\")\n",
    "    label_predicates = [\"http://www.w3.org/2000/01/rdf-schema#label\"] #skip labels\n",
    "    for (s, p, o) in rdf_nb:\n",
    "        if p not in label_predicates:\n",
    "            s_v = Vertex(str(s))\n",
    "            o_v = Vertex(str(o))\n",
    "            p_v = Vertex(str(p), predicate=True, vprev=s_v, vnext=o_v)\n",
    "            kg.add_vertex(s_v)\n",
    "            kg.add_vertex(p_v)\n",
    "            kg.add_vertex(o_v)\n",
    "            kg.add_edge(s_v, p_v)\n",
    "            kg.add_edge(p_v, o_v)\n",
    "    # Construct activity annotation with rules\n",
    "    if use_rules:\n",
    "        with open('activity_rules.json') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            for q in data:\n",
    "                #print('Query: ' + q)\n",
    "                for t in rdf_nb.query(q):\n",
    "                    #print(\"Adding\", t)\n",
    "                    rdf_nb.add(t)\n",
    "    return (kg, rdf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select nodes\n",
    "def selectEntitiesPredict(rdf_nb,kg):\n",
    "    djo = \"http://purl.org/datajourneys/\"\n",
    "    entities = []\n",
    "    for s, p, o in rdf_nb.triples((None, None, None)):\n",
    "        # check if has already an activity\n",
    "        a = rdf_nb.value(s, URIRef(djo + \"hasActivity\"))\n",
    "        if a == None:\n",
    "            entities.append(s)\n",
    "        if (type(o).__name__ == 'URIRef' and p != RDF.type and p != URIRef(djo + \"hasActivity\")):\n",
    "            a = rdf_nb.value(o, URIRef(djo + \"hasActivity\"))\n",
    "            if a == None:\n",
    "                entities.append(o)\n",
    "    entities = list(set(entities))\n",
    "    # Remove the root\n",
    "    root = URIRef('http://purl.org/dj/kaggle/' + notebook)\n",
    "    # typenb = URIRef('http://purl.org/dj/kaggle/Notebook')\n",
    "    entities.remove(root)\n",
    "    # entities.remove(typenb)\n",
    "    # Build embeddings\n",
    "    random_walker = RandomWalker(10, 100) #float('inf')\n",
    "    transformer = RDF2VecTransformer(Word2Vec(sg=0),walkers=[random_walker])\n",
    "    predict = transformer.fit_transform(kg, entities)\n",
    "    return (entities, predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify\n",
    "def classify(clf, predict, entities):\n",
    "    prediction = clf.predict(predict)\n",
    "    activities = list(zip(entities,prediction))\n",
    "    dn_annotated = rdf_nb\n",
    "    djo = \"http://purl.org/datajourneys/\"\n",
    "    for (entity, activity) in activities:\n",
    "        dn_annotated.add((entity, URIRef(djo + \"hasActivity\") , URIRef(djo + activity[1:])))\n",
    "    return dn_annotated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for(s,p,o) in dn_annotated:\n",
    "#     print(s,p,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def n2l(n, g):\n",
    "    if n == None:\n",
    "        return \"None\"\n",
    "    na = getActivity(n, g)\n",
    "    delim = '/'\n",
    "    if '#' in str(n):\n",
    "        delim = '#'\n",
    "    #print(delim, n, g.label(n), na)\n",
    "    nal = \"\"\n",
    "    nl = g.label(n)\n",
    "    if na != None:\n",
    "        nal = \" (\" + g.label(n) + \": \" + str(na).rsplit('/', 1)[1] + \")\"\n",
    "    elif nl != None:\n",
    "        nal = \" (\" + nl + \")\"\n",
    "    return n.rsplit(delim, 1)[1] + nal\n",
    "\n",
    "def getActivity(thisNode, graph):\n",
    "    if(thisNode == None):\n",
    "        return None\n",
    "    REUSE = URIRef(\"http://purl.org/datajourneys/Reuse\")\n",
    "    MOVEMENT = URIRef(\"http://purl.org/datajourneys/Movement\")\n",
    "    ANALYSIS = URIRef(\"http://purl.org/datajourneys/Analysis\")\n",
    "    VISUALISATION = URIRef(\"http://purl.org/datajourneys/Visualisation\")\n",
    "    PREPARATION = URIRef(\"http://purl.org/datajourneys/Preparation\")    \n",
    "    motifs = [REUSE,ANALYSIS,MOVEMENT,VISUALISATION,PREPARATION]\n",
    "    activities = graph.objects(thisNode, URIRef(djo + \"hasActivity\"))\n",
    "    for activity in activities:\n",
    "        if activity in motifs:\n",
    "            return activity\n",
    "    return None\n",
    "\n",
    "# def getSieblings(thisNode, graph):\n",
    "#     sieblings = []\n",
    "#     thisNodeActivity = getActivity(thisNode, graph)\n",
    "#     for prev in graph.subjects(None, thisNode):\n",
    "#         for (predicate, siebling) in graph.predicate_objects(thisNode):\n",
    "#             if predicate == \"http://purl.org/datajourneys/previousActivity\" and siebling != thisNode:\n",
    "#                 sieblingActivity = getActivity(siebling, graph)\n",
    "#                 if(thisNodeActivity == sieblingActivity):\n",
    "#                     sieblings.add(siebling)\n",
    "#     return sieblings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def walkActivities(lastActivityNode, thisNode, graph, d, visited):\n",
    "    if thisNode in visited:\n",
    "        path = \"\"\n",
    "        for v in visited:\n",
    "            path = \"\".join([path , \"/\" , n2l(v, graph)])\n",
    "        print(path + n2l(thisNode, graph) + \"[\" + str(d) + \"]\")\n",
    "        return graph\n",
    "    \n",
    "    visited.append(thisNode)\n",
    "    #print(n2l(lastActivityNode, graph), \" >>> \", n2l(thisNode, graph), \" - \", d)\n",
    "    d=d+1\n",
    "    #print(\"Walking %s (%s)\" % (thisNode, d))\n",
    "    thisActivity = getActivity(thisNode, graph)\n",
    "    lastActivity = getActivity(lastActivityNode, graph)\n",
    "    nextNodes = graph.subjects(None, thisNode)\n",
    "    #     for n in nextNodes:\n",
    "    #         print(\">\",n2l(thisNode, graph), \">>\", n2l(n,graph))\n",
    "    #     print(\"---\")\n",
    "    if(thisActivity == None):\n",
    "        # root here\n",
    "        for n in graph.subjects(None, thisNode):\n",
    "            graph.add((n, URIRef(\"http://purl.org/datajourneys/previousActivity\"), thisNode))\n",
    "            graph = walkActivities(thisNode, n, graph, d, visited.copy())\n",
    "    elif(thisActivity == lastActivity):\n",
    "        for n in graph.subjects(None, thisNode):\n",
    "            graph = walkActivities(lastActivityNode, n, graph, d, visited.copy())\n",
    "    else:\n",
    "        if(lastActivity != None):\n",
    "            graph.add((thisNode, URIRef(\"http://purl.org/datajourneys/previousActivity\"), lastActivityNode))\n",
    "        for n in graph.subjects(None, thisNode):\n",
    "            graph = walkActivities(thisNode, n, graph, d, visited.copy())\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildActivityGraph(lastActivity, lastActivityNode, thisNode, graph, d, visited):\n",
    "    #print(\"Visiting: \", graph.label(thisNode))\n",
    "    if thisNode in visited:\n",
    "        #print(\"Node is visited\")\n",
    "        path = \"\"\n",
    "        for v in visited:\n",
    "            path = \"\".join([path , \"/\" , n2l(v, graph)])\n",
    "        #print(path + n2l(thisNode, graph) + \"[\" + str(d) + \"]\")\n",
    "        return graph\n",
    "    visited.append(thisNode)\n",
    "    d=d+1\n",
    "    #print(\"Walking %s (%s)\" % (thisNode, d))\n",
    "    #sieblings = getSieblings(thisNode, graph)\n",
    "    if(lastActivityNode == None):\n",
    "        # root here\n",
    "        #print(\"Node is root\")\n",
    "        lastActivityNode = thisNode\n",
    "    else:\n",
    "        #print(\"add link to last activity node: \", graph.label(thisNode), graph.label(lastActivityNode))\n",
    "        graph.add((thisNode, URIRef(\"http://purl.org/datajourneys/inActivity\"), lastActivityNode))\n",
    "    # collect next data nodes\n",
    "    nextActivityNodes = {} #URIRef(\"http://purl.org/datajourneys/appearsIn\"),\n",
    "    skipproperties = [ URIRef(\"http://purl.org/datajourneys/previousActivity\"), URIRef(\"http://purl.org/datajourneys/inActivity\"), RDF.type, RDFS.label, URIRef(\"http://purl.org/datajourneys/hasActivity\") ]\n",
    "    for n, p in graph.subject_predicates(thisNode):\n",
    "        if p not in skipproperties:\n",
    "            #print(\" - < property < node\", p, n)\n",
    "            a = getActivity(n, graph)\n",
    "            if a not in nextActivityNodes:\n",
    "                nextActivityNodes[a] = []\n",
    "            nextActivityNodes[a].append(n)\n",
    "    #print(\"Next datanodes:\", nextActivityNodes)\n",
    "    # create an entity for each activity\n",
    "    for a in nextActivityNodes:\n",
    "        #print(\"Checking activity: \", a)\n",
    "        nns = nextActivityNodes.get(a)\n",
    "        #print(\"Nodes in activity \", len(nns), nns)\n",
    "        # If not same activity\n",
    "        if(lastActivity != a):\n",
    "            #print(\"Not the same activity: \", a, lastActivity)\n",
    "            currentActivity = a\n",
    "            # Verify if the nodes involved already are linked to an activity, if yes, reuse that instead of creating a new one\n",
    "            foundActivityNode = None\n",
    "            for ns in nns:\n",
    "                foundActivityNode = graph.value(ns, URIRef(\"http://purl.org/datajourneys/inActivity\"))\n",
    "            if foundActivityNode != None:\n",
    "                #print(\"Found activity in nodeset:\", foundActivityNode)\n",
    "                aent = foundActivityNode\n",
    "            else:\n",
    "                ahash = hash(tuple(nns))\n",
    "                #print(\"Generating activity node: \", str(ahash))\n",
    "                aent = URIRef('http://purl.org/dj/kaggle/' + notebook + \"/activity/\" + str(ahash))\n",
    "                graph.add((aent, RDF.type, currentActivity))\n",
    "                graph.add((aent, RDFS.label, Literal(str(currentActivity)[29:])))\n",
    "            # Link activity to previous\n",
    "            if(lastActivityNode != None):\n",
    "                graph.add((aent, URIRef(\"http://purl.org/datajourneys/previousActivity\"), lastActivityNode))\n",
    "        else:\n",
    "            #print(\"Same activity: \", lastActivity, lastActivityNode)\n",
    "            aent = lastActivityNode\n",
    "            currentActivity = lastActivity\n",
    "        # Add links to datanodes\n",
    "        for n in nns:\n",
    "            #print(\"Graph2: \", graph)\n",
    "            # Walk the dataode path\n",
    "            #print(\"Traverse data node\",n)\n",
    "            #print(\" - activity\",currentActivity)\n",
    "            #print(\" - activity node\",aent)\n",
    "            #print(\" - depth\",d)\n",
    "            graph = buildActivityGraph(currentActivity, aent, n, graph, d, visited.copy())\n",
    "    return graph\n",
    "\n",
    "# def generateDataJourney(dn_annotated, notebook):\n",
    "#     datajourney = dn_annotated\n",
    "#     #http://purl.org/dj/kaggle/very-simple-pytorch-training-0-59#525206600 http://purl.org/dj/appearsIn http://purl.org/dj/kaggle/very-simple-pytorch-training-0-59\n",
    "#     #print(getActivity(URIRef(\"http://purl.org/dj/kaggle/very-simple-pytorch-training-0-59#525206600\"), dn_annotated))\n",
    "#     rootNode = URIRef('http://purl.org/dj/kaggle/' + notebook)\n",
    "#     datajourney = walkActivities(None, rootNode, datajourney, 0, [])\n",
    "#     return datajourney\n",
    "\n",
    "def generateDataJourney(dn_annotated, notebook):\n",
    "    datajourney = dn_annotated\n",
    "    #http://purl.org/dj/kaggle/very-simple-pytorch-training-0-59#525206600 http://purl.org/dj/appearsIn http://purl.org/dj/kaggle/very-simple-pytorch-training-0-59\n",
    "    #print(getActivity(URIRef(\"http://purl.org/dj/kaggle/very-simple-pytorch-training-0-59#525206600\"), dn_annotated))\n",
    "    rootNode = URIRef('http://purl.org/dj/kaggle/' + notebook)\n",
    "    datajourney = buildActivityGraph(None, None, rootNode, datajourney, 0, [])\n",
    "    return datajourney\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# djg = datajourney.triples((None, URIRef(\"http://purl.org/datajourneys/previousActivity\"), None))\n",
    "# for (s,p,o) in djg:\n",
    "#     sa = getActivity(s, datajourney)\n",
    "#     oa = getActivity(o, datajourney)\n",
    "#     print(s,sa,o,oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buildDjDiGraph(g):\n",
    "    gd = networkx.DiGraph()\n",
    "    for (s,p,o) in g.triples((None, URIRef(\"http://purl.org/datajourneys/previousActivity\"), None)):\n",
    "        gd.add_node(s,label=n2l(s,g))\n",
    "        gd.add_node(o,label=n2l(o,g))\n",
    "        gd.add_edge(s,o,label=\"previous\")\n",
    "    return gd\n",
    "\n",
    "def buildDnDigraph(g):\n",
    "    gd = networkx.DiGraph()\n",
    "    for (s,p,o) in g.triples((None, None, None)):\n",
    "        if type(o) == rdflib.term.URIRef and p != RDF.type and p != URIRef(djo + \"hasActivity\") and p != URIRef(djo + \"inActivity\") and p != URIRef(djo + \"previousActivity\"):\n",
    "            gd.add_node(s,label=n2l(s,g))\n",
    "            gd.add_node(o,label=n2l(o,g))\n",
    "            gd.add_edge(s,o,label=n2l(p, g))\n",
    "    return gd\n",
    "    \n",
    "def saveDjDiGraph(fname, gd):\n",
    "    tag = ag.to_agraph(gd)\n",
    "    o = open(outdir + \"/\" + fname + \"_DJ.digraph\", \"w\")\n",
    "    o.write(tag.string())\n",
    "    \n",
    "def saveDnDiGraph(fname, gd):\n",
    "    tag = ag.to_agraph(gd)\n",
    "    o = open(outdir + \"/\" + fname + \"_DN.digraph\", \"w\")\n",
    "    o.write(tag.string())\n",
    "\n",
    "def buildDjPNG(fname):\n",
    "    src = Source.from_file(outdir + \"/\" + fname + \"_DJ.digraph\")\n",
    "    src.format = \"png\"\n",
    "    src.render(outdir + \"/\" + fname + \"_DJ\")\n",
    "\n",
    "def buildDnPNG(fname):\n",
    "    src = Source.from_file(outdir + \"/\" + fname + \"_DN.digraph\")\n",
    "    src.format = \"png\"\n",
    "    src.render(outdir + \"/\" + fname + \"_DN\")\n",
    "\n",
    "def buildDjSVG(fname):\n",
    "    src = Source.from_file(outdir + \"/\" + fname + \"_DJ.digraph\")\n",
    "    src.format = \"svg\"\n",
    "    src.render(outdir + \"/\" + fname + \"_DJ\")\n",
    "\n",
    "def buildDnSVG(fname):\n",
    "    src = Source.from_file(outdir + \"/\" + fname + \"_DN.digraph\")\n",
    "    src.format = \"svg\"\n",
    "    src.render(outdir + \"/\" + fname + \"_DN\")\n",
    "\n",
    "def buildOriginalPNG(fname):\n",
    "    src = Source.from_file(didir + \"/\" + fname + \".digraph\")\n",
    "    src.format = \"png\"\n",
    "    src.render(outdir + \"/\" + fname)\n",
    "    \n",
    "def saveTurtle(fname, datajourney):\n",
    "    datajourney.serialize(destination=outdir + '/' + fname + '.ttl', format='turtle')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFiles(notebook, rdf_nb, datajourney):\n",
    "    dn = buildDnDigraph(rdf_nb)\n",
    "    saveDnDiGraph(notebook, dn)\n",
    "    buildDnPNG(notebook)\n",
    "    buildDnSVG(notebook)\n",
    "    \n",
    "    gd = buildDjDiGraph(datajourney)\n",
    "    saveDjDiGraph(notebook, gd)\n",
    "    buildDjPNG(notebook)\n",
    "    buildDjSVG(notebook)\n",
    "    buildOriginalPNG(notebook)\n",
    "    \n",
    "    saveTurtle(notebook, datajourney)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading classifier\n",
      "Loading classifier [Done 0.001150s]\n",
      "Loading Notebooks\n",
      "Loading Notebooks [Done 53.001183 s]\n"
     ]
    }
   ],
   "source": [
    "# Load the classifier\n",
    "print(\"Loading classifier\")\n",
    "t1 = monotonic()\n",
    "clf = pickle.load(open(clf_folder + '/' + classifier, 'rb'))\n",
    "print(\"Loading classifier [Done %fs]\" % (monotonic()-t1))\n",
    "# The process alltogether\n",
    "print(\"Loading Notebooks\")\n",
    "t1 = monotonic()\n",
    "rdf_graph = loadNotebooks(indir)\n",
    "print(\"Loading Notebooks [Done %f s]\" % (monotonic()-t1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare list of notebooks to process\n",
    "# notebook = 'test'\n",
    "# notebooks = []\n",
    "# if notebook == '*':\n",
    "#     import glob\n",
    "#     for f in glob.glob(indir + \"/*\"):\n",
    "# #         print(f[6:-4])\n",
    "#         notebooks += f\n",
    "# elif type(notebook) == list:\n",
    "#     notebooks = notebook\n",
    "# else:\n",
    "#     notebooks = [notebook]\n",
    "\n",
    "#print(notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each notebook TODO\n",
    "#for notebook in notebooks:\n",
    "# notebook = notebooks[0]\n",
    "# ...\n",
    "print(\"%s : Building KG\" % notebook)\n",
    "t1 = monotonic()\n",
    "kg = loadKG(rdf_graph)\n",
    "print(\"%s : Building KG [Done %f s]\" % (notebook, (monotonic()-t1)))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%s : Loading notebook and add it to the KG\" % notebook)\n",
    "t1 = monotonic()\n",
    "kg,rdf_nb = loadNotebook(indir, kg, notebook, use_rules)\n",
    "print(\"%s : Loading notebook and add it to the KG [Done %f s]\" % (notebook,(monotonic()-t1)))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "print(\"%s : Selecting entities to predict\" % notebook)\n",
    "t1 = monotonic()\n",
    "entities, predict = selectEntitiesPredict(rdf_nb, kg) \n",
    "print(\"%s : Selecting entities to predict [Done %f s]\" % (notebook,(monotonic()-t1)))\n",
    "#\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eda-feature-engineering-lgb-xgb-cat : Building KG\n",
      "eda-feature-engineering-lgb-xgb-cat : Building KG [Done 18.127292 s]\n",
      "eda-feature-engineering-lgb-xgb-cat : Loading notebook and add it to the KG\n",
      "eda-feature-engineering-lgb-xgb-cat : Loading notebook and add it to the KG [Done 0.360544 s]\n",
      "eda-feature-engineering-lgb-xgb-cat : Selecting entities to predict\n",
      "eda-feature-engineering-lgb-xgb-cat : Selecting entities to predict [Done 28.743570 s]\n",
      "eda-feature-engineering-lgb-xgb-cat : Classifying entities\n",
      "eda-feature-engineering-lgb-xgb-cat : Classifying entities [Done 0.018312s]\n"
     ]
    }
   ],
   "source": [
    "print(\"%s : Classifying entities\" % notebook)\n",
    "t1 = monotonic()\n",
    "dn_annotated = classify(clf, predict, entities)\n",
    "print(\"%s : Classifying entities [Done %fs]\" % (notebook,(monotonic()-t1)))\n",
    "#\n",
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%s : Compressing Data Journey\" % notebook)\n",
    "t1 = monotonic()\n",
    "datajourney = generateDataJourney(dn_annotated, notebook)\n",
    "print(\"%s : Compressing Data Journey [Done %fs]\" % (notebook,(monotonic()-t1)))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eda-feature-engineering-lgb-xgb-cat : Compressing Data Journey\n",
      "eda-feature-engineering-lgb-xgb-cat : Compressing Data Journey [Done 2.352714s]\n",
      "eda-feature-engineering-lgb-xgb-cat : Build files\n"
     ]
    }
   ],
   "source": [
    "print(\"%s : Build files\" % notebook)\n",
    "t1 = monotonic()\n",
    "buildFiles(notebook, rdf_nb, datajourney)\n",
    "print(\"%s : Build files [Done %fs]\" % (notebook,(monotonic()-t1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change compression algorithm to include siebling nodes\n",
    "# Support all the notebooks in one execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
